---
title: "Kobe Shot Selection Notebook"
author: "Kelson Jensen"
format: pdf
editor: visual
---

# Intro

This notebook builds machine learning models to predict whether Kobe Bryant made or missed each shot in the Kaggle *Kobe Bryant Shot Selection* competition. The goal is to use the rows that contain known shot outcomes to train several models and then generate predictions for the rows where the outcome is missing.

The workflow includes importing the dataset, splitting into training and test sets, performing exploratory data analysis, engineering new basketball-motivated features, building multiple predictive models, and finally producing a submission file for Kaggle.

### Libraries

These are all of the libraries I needed throughout the document.

```{r}
library(tidyverse)
library(tidymodels)
library(vroom)
library(dplyr)
library(tune)
library(ggplot2)
library(embed)
library(discrim)
library(remotes)
library(xgboost)
```

### Upload data

In this section, the original dataset is loaded and separated into two parts:

-   The **training set** consists of rows where shot_made_flag is not missing.

-   The **test set** consists of rows where shot_made_flag is missing and represents the data for which Kaggle expects predictions.

The training data also recodes shot_made_flag into a factor with levels "missed" and "made" to support classification modeling.

```{r}
# Load your data
df <- vroom("C:/Users/User/OneDrive - Brigham Young University/Fall 2025/STAT 348/kobe-bryant-shot-selection/data.csv/data.csv",  delim = ",")

# Split into training and test sets
train <- df[!is.na(df$shot_made_flag), ]  # Training: where shot_made_flag is not NA
test  <- df[is.na(df$shot_made_flag), ]   # Test: where shot_made_flag is NA

train <- train %>%
  mutate(
    shot_made_flag = ifelse(shot_made_flag == 1, "made", "missed"),
    shot_made_flag = factor(shot_made_flag, levels = c("missed", "made"))
  )

# Check results
nrow(train)
nrow(test)

```

# EDA 

Several visualizations are used to understand Kobe's shooting tendencies:

-   Scatterplots of shot locations on the court using both lon/lat and loc_x/loc_y.

-   A shot chart showing made and missed shots by location.

-   Histograms showing the distribution of shot distances.

-   A bar chart of the number of shots taken in each period.

-   A bar chart of shot success proportions across different shot zone ranges.

These plots help reveal spatial patterns in shot outcomes and motivate feature engineering decisions

```{r}
kobe <- train

ggplot(kobe, aes(x = lon, y = lat)) +
  geom_point(alpha = 0.3, color = "blue") +
  labs(title = "Shot Locations (lon/lat)", x = "Longitude", y = "Latitude") +
  theme_minimal()



ggplot(kobe, aes(x = loc_x, y = loc_y, color = factor(shot_made_flag))) +
  geom_point(alpha = 0.3) +
  scale_color_manual(values = c("red", "green"), labels = c("Missed", "Made")) +
  labs(title = "Shot Locations by Outcome", color = "Shot Made?") +
  theme_minimal()


ggplot(kobe, aes(x = shot_distance)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Shot Distances", x = "Distance (feet)", y = "Count") +
  theme_minimal()


ggplot(kobe, aes(x = period)) +
  geom_bar(fill = "darkcyan") +
  labs(title = "Shots by Game Period", x = "Period", y = "Number of Shots") +
  theme_minimal()


ggplot(kobe, aes(x = shot_zone_range, fill = factor(shot_made_flag))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("red", "green"),
                    labels = c("Missed", "Made")) +
  labs(title = "Shot Accuracy by Shot Zone Range", 
       x = "Zone Range", y = "Success Proportion") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Recipe

To improve model performance, several new variables are created based on basketball context and court geometry.

Key engineered features include:

-    **Home vs Away:** Determined from whether the matchup string contains "vs.".

-    **Season short code:** Extracted from the season range (e.g., "2009-10" â†’ 0).

-    **Game year:** Extracted from the game date.

-    **Game number:** A numeric version of game_date used to model trends over time.

-    **Post-Achilles injury indicator:** Whether the shot occurred after Kobeâ€™s Achilles injury.

-    **MVP period indicator:** Whether it occurred during a chosen MVP-related window.

-    **Time remaining in the period:** Total seconds left when the shot was taken.

-   **Polar coordinates:**

    -    Shot distance computed from loc_x and loc_y.

    -    Shot angle using atan2 to correctly capture shooting direction.

The recipe also handles missing values, converts categorical variables to dummy variables, removes zero-variance predictors, and standardizes numeric predictors.

```{r}
my_recipe <- recipe(shot_made_flag ~ ., data = train) %>%
  
  ## ---- Remove leakage / non-predictive identifiers ----
  step_rm(shot_id, game_id, team_id, game_event_id, team_name) %>%
  
  ## ---- Basic feature engineering ----
  # Time remaining in the period (seconds)
  step_mutate(
    time_remaining = minutes_remaining * 60 + seconds_remaining
  ) %>%
  
  # Home vs Away indicator from "matchup"
  step_mutate(
    home = ifelse(str_detect(matchup, "vs"), 1, 0)
  ) %>%
  
  # Season start year (e.g. "2000-01" â†’ 2000)
  step_mutate(
    season_start = as.integer(str_sub(season, 1, 4))
  ) %>%
  
  ## ---- Date features ----
  step_mutate(
    game_month = month(game_date),
    game_year  = year(game_date),
    game_num   = as.numeric(as.Date(game_date))   # needed for postachilles / mvp
  ) %>%
  
  ## ---- Polar coordinates + spatial features ----
  step_mutate(
    shot_distance = sqrt((loc_x / 10)^2 + (loc_y / 10)^2),
    angle         = atan2(loc_y, loc_x),
    left_right    = ifelse(loc_x < 0, "Left", "Right"),
    dist_bucket   = case_when(
      shot_distance <= 5  ~ "0-5ft",
      shot_distance <= 15 ~ "6-15ft",
      shot_distance <= 24 ~ "16-24ft",
      TRUE                ~ "25+ft"
    ),
    angle_quadrant = case_when(
      angle >= -pi/4  & angle <=  pi/4   ~ "Center",
      angle >   pi/4  & angle <= 3*pi/4  ~ "Left",
      TRUE                               ~ "Right"
    )
  ) %>%
  
  ## ---- Game-era / clutch context ----
  step_mutate(
    postachilles = ifelse(game_num > 1452, 1, 0),
    mvp         = ifelse(game_num >= 909 & game_num <= 990, 1, 0),
    clutch      = ifelse(time_remaining <= 24, 1, 0),
    deep_clutch = ifelse(period == 4 & time_remaining <= 24, 1, 0)
  ) %>%
  
  ## ---- Drop raw columns now replaced by engineered features ----
  step_rm(
    game_date,
    season,
    matchup,
    opponent,
    loc_x,
    loc_y
  ) %>%
  
  ## ---- Clean categorical variables ----
  # Convert character â†’ factor
  step_string2factor(all_nominal_predictors()) %>%
  
  # One-hot encode 
  step_dummy(all_nominal_predictors()) %>%
  
  ## ---- Handle missing values ----
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  
  ## ---- Normalize numeric predictors ----
  step_normalize(all_numeric_predictors()) %>%
  
  ## ---- Remove zero-variance predictors ----
  step_zv(all_predictors())


shot_prep <- prep(my_recipe)
train_ready <- juice(shot_prep)
test_ready <- bake(shot_prep, new_data = test)
```

### Test recipes 

The recipe above was not working to get my predictions as accurate as I was hoping for. So I resulted to a more simple version that made better predictions. In this recipe I took our leakage variables, adjusted basic features, normalized, and dummy encoded the variables.

```{r}
my_recipe <- recipe(shot_made_flag ~ ., data = train) %>%
  
  ## ---- Remove leakage / non-predictive identifiers ----
step_rm(shot_id,game_id, team_id, game_event_id, team_name) %>%
  
  ## ---- Basic feature engineering ----
# Time remaining in the period
step_mutate(time_remaining = minutes_remaining * 60 + seconds_remaining) %>%
  
  # Home vs Away indicator from "matchup"
  step_mutate(home = ifelse(str_detect(matchup, "vs"), 1, 0)) %>%
  
  # Extract season start year (e.g. "2000-01" â†’ 2000)
  step_mutate(season_start = as.integer(str_sub(season, 1, 4))) %>%
  
  ## ---- Date features ----
step_mutate(
  game_month = month(game_date),
  game_year  = year(game_date)
) %>%
  step_rm(game_date) %>%  # remove raw date afterward
  
  ## ---- Clean categorical variables ----
# Convert character â†’ factor
step_string2factor(all_nominal_predictors()) %>%
  
  # One-hot encode 
  step_dummy(all_nominal_predictors()) %>%
  
  ## ---- Handle missing values ----
step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  
  ## ---- Normalize numeric predictors ----
step_normalize(all_numeric_predictors()) %>%
  
  ## ---- Remove zero-variance predictors ----
step_zv(all_predictors()) 
```

# RF Model (Tuned)

A random forest classifier is built using the engineered features.\
Two hyperparameters are tuned using cross-validation:

-    mtry: Number of predictors sampled at each split.

-    min_n: Minimum observations required in each terminal node.

A small regular grid is used to keep computation light.\
The best model is selected based on ROC AUC.\
The tuned model is then fit to the entire training set and used to generate predictions for the test set.

```{r}
rf_spec <- rand_forest(
  mtry  = tune(),
  min_n = tune(),
  trees = 500          # fixed number of trees (no tuning)
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

# 5) Workflow
rf_wf <- workflow() %>%
  add_recipe(my_recipe) %>%
  add_model(rf_spec)

# 6) CV folds (small to keep it light)
folds <- vfold_cv(train, v = 3, strata = shot_made_flag)

# 7) Simple tuning grid for mtry & min_n
mtry_range  <- finalize(mtry(), train_ready)              # based on processed predictors
min_n_range <- min_n(range = c(5L, 20L))                  # small nodes

rf_grid <- grid_regular(
  mtry_range,
  min_n_range,
  levels = 3           # 3 x 3 = 9 combos => cheap
)

# 8) Tune
tuned_rf <- tune_grid(
  rf_wf,
  resamples = folds,
  grid      = rf_grid,
  metrics   = metric_set(roc_auc, accuracy)
)

best_rf <- select_best(tuned_rf, metric = "roc_auc")

final_rf_wf <- finalize_workflow(rf_wf, best_rf)

# 9) Fit tuned RF on full training data
final_fit <- fit(final_rf_wf, data = train)

# 10) Predict on test set (no shot_made_flag in test)
test_prob  <- predict(final_fit, new_data = test, type = "prob")
test_class <- predict(final_fit, new_data = test, type = "class")

test_preds <- bind_cols(
  test %>% select(shot_id),
  tibble(
    prob_made = test_prob$.pred_made,
    pred_flag = test_class$.pred_class
  )
)

# 11) Submission file
submission <- test_preds %>%
  transmute(
    shot_id,
    shot_made_flag = prob_made
  )

write.csv(submission, "kobe-submission-tuned-simple-rf.csv", row.names = FALSE)
```

# RF Model Not tuned

A baseline random forest model is also fit without tuning to compare against the tuned version.
Here, fixed values of mtry, min_n, and trees are chosen manually. The model is trained on the full training data and used to produce predictions for the test set.

```{r}
prep_obj <- prep(my_recipe)
train_ready <- juice(prep_obj)
p <- ncol(train_ready) - 1  # number of predictors
floor(sqrt(p))


rf_spec <- rand_forest(
  mtry  = 20,   # better default than 10
  min_n = 40,                # still reasonable
  trees = 500              # stronger forest
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "permutation")

rf_wf <- workflow() %>%
  add_recipe(my_recipe) %>%
  add_model(rf_spec)

final_fit <- fit(rf_wf, data = train)

test_prob  <- predict(final_fit, new_data = test, type = "prob")
test_class <- predict(final_fit, new_data = test, type = "class")

test_preds <- bind_cols(
  test %>% select(shot_id),
  tibble(
    prob_made = test_prob$.pred_made,
    pred_flag = test_class$.pred_class
  )
)

submission <- test_preds %>%
  transmute(
    shot_id,
    shot_made_flag = prob_made
  )

write.csv(submission, "kobe-submission.csv", row.names = FALSE)

```

# Penalized regression 

A penalized logistic regression model is built using glmnet.\
Two parameters are tuned:

-    penalty: Controls overall regularization strength.

-    mixture: Controls the blend between ridge regression (0), lasso (1), or elastic net (between).

A regular grid over both parameters is evaluated using cross-validation.\
The best model is finalized and used to generate predictions.

This model provides an interpretable linear alternative to the tree-based models.

```{r}
log_spec <- logistic_reg(
  penalty = tune(),   # lambda
  mixture = tune()    # 0 = ridge, 1 = lasso, in between = elastic net
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

log_wf <- workflow() %>%
  add_recipe(my_recipe) %>%
  add_model(log_spec)

# 4) Cross-validation + tuning grid
folds <- vfold_cv(train, v = 5, strata = shot_made_flag)

log_grid <- grid_regular(
  penalty(range = c(-4, 1)),   # 10^-4 to 10^1
  mixture(range = c(0, 1)),    # ridge -> elastic -> lasso
  levels = 7
)

tuned_log <- tune_grid(
  log_wf,
  resamples = folds,
  grid = log_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# ðŸ”§ FIX IS HERE
best_log <- select_best(tuned_log, metric = "roc_auc")

final_log_wf <- finalize_workflow(log_wf, best_log)

# 5) Fit final model
final_log_fit <- fit(final_log_wf, data = train)

# 6) Predict on test (no shot_made_flag in `test`)
test_prob  <- predict(final_log_fit, new_data = test, type = "prob")
test_class <- predict(final_log_fit, new_data = test, type = "class")

test_preds <- bind_cols(
  test %>% select(shot_id),
  tibble(
    prob_made = test_prob$.pred_made,
    pred_flag = test_class$.pred_class
  )
)

# 7) Optional: submission file
submission <- test_preds %>%
  transmute(
    shot_id,
    shot_made_flag = prob_made
  )

write.csv(submission, "submission_glmnet.csv", row.names = FALSE)
```

# XGboost model

An XGBoost gradient boosted tree model is trained using the engineered features. A simple but effective configuration is used rather than a full hyperparameter search:

-    1000 trees

-    Tree depth of 6

-    Learning rate of 0.05

-    Row sampling of 0.8

-    Feature sampling (mtry) of 0.8, treated as a proportion

XGBoost often performs well on structured tabular data, and this model is used to generate final predictions for the competition.

```{r}
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = 6,
  min_n = 10,
  learn_rate = 0.05,
  loss_reduction = 0,
  sample_size = 0.8,   # row subsampling
  mtry = 0.8           # feature subsampling (proportion)
) %>%
  set_engine("xgboost", counts = FALSE) %>%   # <-- FIX HERE
  set_mode("classification")

xgb_wf <- workflow() %>%
  add_recipe(my_recipe) %>%
  add_model(xgb_spec)

xgb_fit <- fit(xgb_wf, data = train)

test_prob  <- predict(xgb_fit, new_data = test, type = "prob")
test_class <- predict(xgb_fit, new_data = test, type = "class")

test_preds <- bind_cols(
  test %>% select(shot_id),
  tibble(
    prob_made = test_prob$.pred_made,
    pred_flag = test_class$.pred_class
  )
)

submission <- test_preds %>%
  transmute(
    shot_id,
    shot_made_flag = prob_made
  )

write.csv(submission, "kobe-xgboost-simple.csv", row.names = FALSE)
```

### Prediction generation

Each model generates predicted class probabilities for the test set.\
A submission file is created by selecting:

-    shot_id

-    The corresponding predicted probability of shot_made_flag = "made"

The file is written as a CSV and uploaded to Kaggle for scoring.

# Conclusion

This notebook explored several modeling techniques for predicting shot outcomes in the Kobe Bryant dataset.\
Through EDA, feature engineering, random forest models, penalized regression, and XGBoost, we built and compared multiple approaches.\
The final output is a set of predictions formatted for submission to the Kaggle competition.
